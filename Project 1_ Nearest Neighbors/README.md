K-Nearest Neighbors Classification: From Iris Clustering to Census Income Prediction

In this module, I worked extensively with the k-nearest neighbors (KNN) algorithm to build a strong foundation in both its mathematical concepts and its practical applications. I began with a midweek project where I applied KNN to the classic Iris dataset, walking through the full process of loading the data, splitting it into training and test sets, visualizing the clusters, and implementing the algorithm manually. Through this exercise, I achieved an overall classification accuracy of 92 percent, with perfect accuracy for two of the classes. This project helped me understand how KNN uses distance-based relationships to make predictions, how clustering patterns influence model performance, and what makes KNN effective for well-separated datasets.

For the main Module 1 project, I expanded these skills by cleaning, visualizing, and modeling a much larger and more complex census dataset to classify individuals into low-income and high-income categories. I performed comprehensive data preprocessing, including handling missing values, encoding categorical variables, standardizing features, and visualizing demographic and economic patterns. I then optimized a KNN model using GridSearchCV, ultimately reaching an accuracy of 83.28%. This project deepened my understanding of feature scaling, hyperparameter tuning, and interpreting class-level precision and recall in real-world socioeconomic data. Together, these projects strengthened my ability to prepare data, build KNN models, communicate findings to stakeholders, and translate predictive results into actionable insights.
